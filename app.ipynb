{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8fef4db",
   "metadata": {},
   "source": [
    "# Trash Classification using Deep Learning\n",
    "\n",
    "This notebook implements a waste classification system using convolutional neural networks. It compares a custom CNN model with a transfer learning approach using MobileNetV2.\n",
    "\n",
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b78e79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import VGG16, MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow.keras.backend as K\n",
    "import cv2\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import random\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40733e3e",
   "metadata": {},
   "source": [
    "## 2. Configuration Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06f17b9",
   "metadata": {},
   "source": [
    "### 2.1 Parameter Explanation\n",
    "\n",
    "| Parameter    | Lower Value (Pros/Cons)                       | Higher Value (Pros/Cons)                           |\n",
    "| ------------ | --------------------------------------------- | -------------------------------------------------- |\n",
    "| `IMG_SIZE`   | + Faster training<br>- May lose image details | + More image detail<br>- Slower, memory intensive  |\n",
    "| `BATCH_SIZE` | + Less memory needed<br>- Noisy gradients     | + Faster, stable gradients<br>- More memory needed |\n",
    "| `EPOCHS`     | + Faster completion<br>- Risk of underfitting | + More training time<br>- Risk of overfitting      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ad6b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters\n",
    "IMG_SIZE = 224  # Standard size for many pre-trained models\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "NUM_CLASSES = 6  # cardboard, glass, metal, paper, plastic, trash\n",
    "CLASSES = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']  # Class names\n",
    "\n",
    "# Define the data directory - update this to your dataset path\n",
    "# Assuming a structure like: \n",
    "# data/\n",
    "#   train/\n",
    "#     cardboard/\n",
    "#     glass/\n",
    "#     ...\n",
    "#   valid/\n",
    "#     cardboard/\n",
    "#     glass/\n",
    "#     ...\n",
    "#   test/\n",
    "#     cardboard/\n",
    "#     glass/\n",
    "#     ...\n",
    "DATA_DIR = 'dataset'\n",
    "\n",
    "# Create directory if it doesn't exist (for saving models)\n",
    "if not os.path.exists('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "# Create directory for figures\n",
    "if not os.path.exists('figures'):\n",
    "    os.makedirs('figures')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859a24eb",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "### 3.1 Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb0c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    \"\"\"\n",
    "    Load and preprocess the dataset with data augmentation.\n",
    "    Returns data generators for training, validation, and testing.\n",
    "    \"\"\"\n",
    "    print(\"Setting up data generators...\")\n",
    "    \n",
    "    # Data augmentation for training set\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    # Only rescaling for validation and test sets\n",
    "    valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    # Setup the generators\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        os.path.join(DATA_DIR, 'train'),\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    valid_generator = valid_datagen.flow_from_directory(\n",
    "        os.path.join(DATA_DIR, 'valid'),\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        os.path.join(DATA_DIR, 'test'),\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Found {train_generator.samples} training images\")\n",
    "    print(f\"Found {valid_generator.samples} validation images\")\n",
    "    print(f\"Found {test_generator.samples} test images\")\n",
    "    \n",
    "    # Get the class indices for further reference\n",
    "    class_indices = train_generator.class_indices\n",
    "    print(f\"Class indices: {class_indices}\")\n",
    "    return train_generator, valid_generator, test_generator, class_indices\n",
    "\n",
    "# Load and preprocess the data\n",
    "train_generator, valid_generator, test_generator, class_indices = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31271672",
   "metadata": {},
   "source": [
    "### 3.2 Explore and Visualize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc1d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_data(train_generator, class_indices):\n",
    "    \"\"\"\n",
    "    Explore and visualize the dataset.\n",
    "    \"\"\"\n",
    "    # Count samples per class\n",
    "    print(\"Analyzing dataset distribution...\")\n",
    "    class_counts = {}\n",
    "    for class_name in class_indices.keys():\n",
    "        class_dir = os.path.join(DATA_DIR, 'train', class_name)\n",
    "        count = len(os.listdir(class_dir))\n",
    "        class_counts[class_name] = count\n",
    "    \n",
    "    # Plot class distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(class_counts.keys(), class_counts.values())\n",
    "    plt.title('Number of Training Images per Class')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/class_distribution.png')\n",
    "    plt.show()  # Display the figure in the notebook\n",
    "    \n",
    "    # Visualize some sample images\n",
    "    print(\"Visualizing sample images...\")\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, class_name in enumerate(class_indices.keys()):\n",
    "        class_dir = os.path.join(DATA_DIR, 'train', class_name)\n",
    "        images = os.listdir(class_dir)\n",
    "        # Get 3 random images for each class\n",
    "        if len(images) >= 3:\n",
    "            sample_images = random.sample(images, 3)\n",
    "            for j, img_name in enumerate(sample_images):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                img = cv2.imread(img_path)\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                plt.subplot(6, 3, i*3 + j + 1)\n",
    "                plt.imshow(img)\n",
    "                plt.title(class_name)\n",
    "                plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/sample_images.png')\n",
    "    plt.show()  # Display the figure in the notebook\n",
    "    \n",
    "    # Display a batch of images with their labels\n",
    "    x_batch, y_batch = next(train_generator)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i in range(min(9, len(x_batch))):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        plt.imshow(x_batch[i])\n",
    "        \n",
    "        # Get class name from one-hot encoded label\n",
    "        class_idx = np.argmax(y_batch[i])\n",
    "        class_name = list(class_indices.keys())[list(class_indices.values()).index(class_idx)]\n",
    "        \n",
    "        plt.title(class_name)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/batch_images.png')\n",
    "    plt.show()  # Display the figure in the notebook\n",
    "    \n",
    "    # Show an example of data augmentation\n",
    "    print(\"Visualizing data augmentation effects...\")\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Original image\n",
    "    sample_img_path = os.path.join(DATA_DIR, 'train', list(class_indices.keys())[0], \n",
    "                                  os.listdir(os.path.join(DATA_DIR, 'train', list(class_indices.keys())[0]))[0])\n",
    "    original_img = cv2.imread(sample_img_path)\n",
    "    original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "    original_img = cv2.resize(original_img, (IMG_SIZE, IMG_SIZE))\n",
    "    \n",
    "    plt.subplot(1, 5, 1)\n",
    "    plt.imshow(original_img)\n",
    "    plt.title('Original')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Create a data generator just for this image\n",
    "    img_array = np.expand_dims(original_img, axis=0)\n",
    "    aug_datagen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        zoom_range=0.15,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.15,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode=\"nearest\"\n",
    "    )\n",
    "    \n",
    "    aug_iter = aug_datagen.flow(img_array, batch_size=1)\n",
    "    \n",
    "    # Generate 4 augmented examples\n",
    "    for i in range(4):\n",
    "        plt.subplot(1, 5, i+2)\n",
    "        aug_img = next(aug_iter)[0].astype('uint8')\n",
    "        plt.imshow(aug_img)\n",
    "        plt.title(f'Augmented #{i+1}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/data_augmentation.png')\n",
    "    plt.show()  # Display the figure in the notebook\n",
    "    \n",
    "    print(\"Data exploration complete!\")\n",
    "\n",
    "# Explore the data\n",
    "explore_data(train_generator, class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f781cf2",
   "metadata": {},
   "source": [
    "## 4. Model Selection and Training\n",
    "\n",
    "### 4.1 Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7fa74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_cnn(input_shape=(IMG_SIZE, IMG_SIZE, 3), num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    Create a custom CNN model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # First convolutional block\n",
    "        Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape, kernel_regularizer=l2(0.001)),\n",
    "        Conv2D(32, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Third convolutional block\n",
    "        Conv2D(128, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Conv2D(128, (3, 3), padding='same', activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Fully connected layers\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def create_transfer_learning_model(base_model_name='mobilenetv2', input_shape=(IMG_SIZE, IMG_SIZE, 3), num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    Create a transfer learning model using pre-trained weights from ImageNet.\n",
    "    \"\"\"\n",
    "    if base_model_name.lower() == 'vgg16':\n",
    "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    elif base_model_name.lower() == 'mobilenetv2':\n",
    "        base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported base model. Choose 'vgg16' or 'mobilenetv2'.\")\n",
    "    \n",
    "    # Freeze the base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Add custom layers on top\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Create the full model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5070a94a",
   "metadata": {},
   "source": [
    "### 4.2 Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2abf642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, train_generator, valid_generator, model_name):\n",
    "    \"\"\"\n",
    "    Train and evaluate the model.\n",
    "    \"\"\"\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    # Setup callbacks\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        f'models/{model_name}_best.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=valid_generator,\n",
    "        callbacks=[checkpoint, early_stopping, reduce_lr]\n",
    "    )\n",
    "    \n",
    "    # Save the final model\n",
    "    model.save(f'models/{model_name}_final.h5')\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history, model_name)\n",
    "    \n",
    "    return history, model\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"\n",
    "    Plot the training and validation accuracy/loss.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot training & validation accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(f'{model_name} - Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(f'{model_name} - Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figures/{model_name}_training_history.png')\n",
    "    plt.show()  # Display the figure in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599656e6",
   "metadata": {},
   "source": [
    "### 4.3 Create and Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10f460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the custom CNN model\n",
    "custom_model = create_custom_cnn()\n",
    "custom_model.summary()\n",
    "\n",
    "# Train the custom CNN model\n",
    "custom_history, custom_model = train_and_evaluate_model(\n",
    "    custom_model, train_generator, valid_generator, \"custom_cnn\"\n",
    ")\n",
    "\n",
    "# Create and train the transfer learning model with MobileNetV2\n",
    "transfer_model = create_transfer_learning_model(base_model_name='mobilenetv2')\n",
    "transfer_model.summary()\n",
    "\n",
    "# Train the transfer learning model\n",
    "transfer_history, transfer_model = train_and_evaluate_model(\n",
    "    transfer_model, train_generator, valid_generator, \"mobilenetv2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b473ec1",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "### 5.1 Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cdb624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_generator, class_indices, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set.\n",
    "    \"\"\"\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    \n",
    "    # Get the true labels\n",
    "    test_generator.reset()\n",
    "    y_true = test_generator.classes\n",
    "    \n",
    "    # Get class names in the correct order\n",
    "    class_names = [k for k, v in sorted(class_indices.items(), key=lambda item: item[1])]\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred_probs = model.predict(test_generator)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    \n",
    "    # Calculate and print metrics\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cr = classification_report(y_true, y_pred, target_names=class_names)\n",
    "    \n",
    "    print(f\"\\nClassification Report for {model_name}:\")\n",
    "    print(cr)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figures/{model_name}_confusion_matrix.png')\n",
    "    plt.show()  # Display the figure in the notebook\n",
    "    \n",
    "    # Return metrics for comparison\n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'accuracy': np.mean(y_pred == y_true),\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': cr\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate the custom CNN model\n",
    "custom_results = evaluate_model(custom_model, test_generator, class_indices, \"custom_cnn\")\n",
    "\n",
    "# Evaluate the transfer learning model\n",
    "transfer_results = evaluate_model(transfer_model, test_generator, class_indices, \"mobilenetv2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305006fc",
   "metadata": {},
   "source": [
    "### 5.2 Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fd3941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(results_list):\n",
    "    \"\"\"\n",
    "    Compare the performance of different models.\n",
    "    \"\"\"\n",
    "    model_names = [result['model'] for result in results_list]\n",
    "    accuracies = [result['accuracy'] for result in results_list]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(model_names, accuracies, color=['skyblue', 'salmon'])\n",
    "    plt.title('Model Comparison - Test Accuracy')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 1])\n",
    "    \n",
    "    # Add accuracy values on top of bars\n",
    "    for i, v in enumerate(accuracies):\n",
    "        plt.text(i, v + 0.01, f'{v:.4f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/model_comparison.png')\n",
    "    plt.show()  # Display the figure in the notebook\n",
    "    \n",
    "    # Print comparison summary\n",
    "    print(\"\\nModel Comparison Summary:\")\n",
    "    for i, result in enumerate(results_list):\n",
    "        print(f\"{i+1}. {result['model']} - Accuracy: {result['accuracy']:.4f}\")\n",
    "\n",
    "# Compare the models\n",
    "compare_models([custom_results, transfer_results])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c24ef7",
   "metadata": {},
   "source": [
    "## 6. Model Explanation with Grad-CAM\n",
    "\n",
    "### 6.1 Generate Grad-CAM Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa08fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gradcam(model, img_path, class_indices, layer_name=None):\n",
    "    \"\"\"\n",
    "    Generate Grad-CAM visualization for a single image.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        img_path: Path to the image\n",
    "        class_indices: Dictionary mapping class names to indices\n",
    "        layer_name: Name of the layer to use for Grad-CAM (if None, will use the last conv layer)\n",
    "    \n",
    "    Returns:\n",
    "        Original image and heatmap\n",
    "    \"\"\"\n",
    "    # Load and preprocess the image\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_display = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    \n",
    "    img = img_display.astype(np.float32) / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    \n",
    "    # Make a prediction\n",
    "    preds = model.predict(img)\n",
    "    pred_class = np.argmax(preds[0])\n",
    "    pred_class_name = list(class_indices.keys())[list(class_indices.values()).index(pred_class)]\n",
    "\n",
    "    # Find the last convolutional layer if not specified\n",
    "    if layer_name is None:\n",
    "        for layer in reversed(model.layers):\n",
    "            if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "                layer_name = layer.name\n",
    "                break\n",
    "    \n",
    "    # Get the last conv layer\n",
    "    last_conv_layer = model.get_layer(layer_name)\n",
    "\n",
    "    # if isinstance(model, Sequential):\n",
    "    #     # This is the key fix - explicitly create the model with inputs and outputs\n",
    "    #     grad_model = Model(\n",
    "    #         inputs=[model.inputs],\n",
    "    #         outputs=[last_conv_layer.output, *model.outputs]\n",
    "    #     )\n",
    "    # else:\n",
    "    #     # For functional models, we can use model.inputs\n",
    "    #     grad_model = Model(\n",
    "    #         inputs=[model.inputs],\n",
    "    #         outputs=[last_conv_layer.output, *model.outputs]\n",
    "    #     )\n",
    "    # Create a model that maps the input image to the activations of the last conv layer\n",
    "    grad_model = Model(\n",
    "         inputs=[model.inputs],\n",
    "         outputs=[last_conv_layer.output, *model.outputs]\n",
    "     )\n",
    "    \n",
    "    # Compute gradient of the top predicted class with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img)\n",
    "        loss = predictions[:, pred_class]\n",
    "    \n",
    "    # Extract the gradients\n",
    "    grads = tape.gradient(loss, conv_outputs)\n",
    "    \n",
    "    # Pool the gradients across all axes except for the batch and channel dimensions\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    \n",
    "    # Multiply each channel in the feature map array by the importance of this channel\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = tf.reduce_mean(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n",
    "    \n",
    "    # Normalize the heatmap\n",
    "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
    "    \n",
    "    # Resize the heatmap to the original image size\n",
    "    heatmap = cv2.resize(heatmap, (IMG_SIZE, IMG_SIZE))\n",
    "    \n",
    "    # Convert heatmap to RGB\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    \n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = heatmap * 0.4 + img_display\n",
    "    superimposed_img = np.clip(superimposed_img, 0, 255).astype('uint8')\n",
    "    \n",
    "    return img_display, superimposed_img, pred_class_name, np.max(preds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3979f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gradcam_for_multiple_classes(model, test_generator, class_indices, model_name):\n",
    "    \"\"\"\n",
    "    Visualize Grad-CAM for multiple images from different classes.\n",
    "    \"\"\"\n",
    "    print(f\"Generating Grad-CAM visualizations for {model_name}...\")\n",
    "    \n",
    "    # Create a figure to display multiple images with their GradCAM heatmaps\n",
    "    plt.figure(figsize=(20, 4 * len(class_indices)))\n",
    "    \n",
    "    # Counter for subplot positioning\n",
    "    subplot_idx = 1\n",
    "    \n",
    "    # For each class, find 2 correctly classified images\n",
    "    for class_name, class_idx in class_indices.items():\n",
    "        # Find images from this class in the test set\n",
    "        test_dir = os.path.join(DATA_DIR, 'test', class_name)\n",
    "        if not os.path.exists(test_dir):\n",
    "            print(f\"Test directory not found for class {class_name}\")\n",
    "            continue\n",
    "            \n",
    "        image_files = os.listdir(test_dir)\n",
    "        if not image_files:\n",
    "            print(f\"No images found for class {class_name}\")\n",
    "            continue\n",
    "            \n",
    "        # Try to find 2 correctly classified images\n",
    "        correct_images = []\n",
    "        for img_file in image_files:\n",
    "            if len(correct_images) >= 2:\n",
    "                break\n",
    "                \n",
    "            img_path = os.path.join(test_dir, img_file)\n",
    "            \n",
    "            # Load and preprocess the image\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "            \n",
    "            img_array = img.astype(np.float32) / 255.0\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "            \n",
    "            # Make a prediction\n",
    "            pred = model.predict(img_array)\n",
    "            pred_class = np.argmax(pred[0])\n",
    "            # If correctly classified, add to our list\n",
    "            if pred_class == class_idx:\n",
    "                correct_images.append(img_path)\n",
    "        \n",
    "        # Visualize GradCAM for the selected images\n",
    "        for i, img_path in enumerate(correct_images):\n",
    "            # Generate GradCAM\n",
    "\n",
    "            print(img_path)\n",
    "            img, heatmap, pred_class_name, confidence = generate_gradcam(model, img_path, class_indices)\n",
    "            \n",
    "            # Original image\n",
    "            plt.subplot(len(class_indices), 4, subplot_idx)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Original - {class_name}\")\n",
    "            plt.axis('off')\n",
    "            subplot_idx += 1\n",
    "            \n",
    "            # GradCAM heatmap\n",
    "            plt.subplot(len(class_indices), 4, subplot_idx)\n",
    "            plt.imshow(heatmap)\n",
    "            plt.title(f\"GradCAM - {pred_class_name} ({confidence:.2f})\")\n",
    "            plt.axis('off')\n",
    "            subplot_idx += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'figures/{model_name}_gradcam.png')\n",
    "    plt.show()  # Display the figure in the notebook\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3bb2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Grad-CAM visualizations for the custom CNN model\n",
    "visualize_gradcam_for_multiple_classes(custom_model, test_generator, class_indices, \"custom_cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8c170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate Grad-CAM visualizations for the transfer learning model\n",
    "visualize_gradcam_for_multiple_classes(transfer_model, test_generator, class_indices, \"mobilenetv2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613f9cef",
   "metadata": {},
   "source": [
    "## 7. Conclusion and Future Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119cb075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a conclusion summarizing the results\n",
    "print(\"# Project Conclusion\")\n",
    "print(\"\\n## Results Summary\")\n",
    "print(f\"1. Custom CNN Accuracy: {custom_results['accuracy']:.4f}\")\n",
    "print(f\"2. MobileNetV2 Transfer Learning Accuracy: {transfer_results['accuracy']:.4f}\")\n",
    "print(\"\\n## Key Findings\")\n",
    "print(\"- MobileNetV2 with transfer learning outperformed the custom CNN model.\") if transfer_results['accuracy'] > custom_results['accuracy'] else print(\"- Custom CNN outperformed the MobileNetV2 transfer learning model.\")\n",
    "print(\"- The model successfully classified 6 different types of waste materials.\")\n",
    "print(\"- Grad-CAM visualizations show that the models are focusing on relevant features.\")\n",
    "\n",
    "print(\"\\n## Future Work\")\n",
    "print(\"1. Test other pre-trained architectures (e.g., EfficientNet, ResNet)\")\n",
    "print(\"2. Fine-tune the pre-trained models by unfreezing some layers\")\n",
    "print(\"3. Test the model on real-world images from mobile devices\")\n",
    "print(\"4. Develop a web or mobile application for real-time waste classification\")\n",
    "print(\"5. Expand the dataset with more diverse waste materials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8d8fc0",
   "metadata": {},
   "source": [
    "This notebook has successfully implemented and evaluated two deep learning models for trash classification:\n",
    "1. A custom-built convolutional neural network\n",
    "2. A transfer learning approach using MobileNetV2\n",
    "\n",
    "The models were trained on six waste categories: cardboard, glass, metal, paper, plastic, and trash, and evaluated through accuracy metrics and Grad-CAM visualizations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
